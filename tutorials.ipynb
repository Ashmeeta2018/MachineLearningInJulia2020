{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Julia, JuliaCon2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A workshop introducing the machine learning toolbox\n",
    "[MLJ](https://alan-turing-institute.github.io/MLJ.jl/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `include` call loads a Julia environment, defined in\n",
    "Project.toml and Manifest.toml files **which must be in the same\n",
    "directory as this file**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(joinpath(@__DIR__, \"setup.jl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is the notebook version of the tutorial, then it is\n",
    "recommended that you clear all cell outputs before attempting the\n",
    "tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Goals:**\n",
    "> 1. Learn how MLJ specifies it's data requirements using \"scientific\" types\n",
    "> 2. Understand the options for representing tabular data\n",
    "> 3. Learn how to inspect and fix the representation of data to meet MLJ requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scientific types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you focus on the intended *purpose* or *interpretation* of\n",
    "data, MLJ models specify data requirements using *scientific types*,\n",
    "instead of machine types. An example of a scientific type is\n",
    "`OrderedFactor`. The other basic \"scalar\" scientific types are\n",
    "illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/scitypes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scientific type is an ordinary Julia type (so it can be used for\n",
    "method dispatch, for example) but it usually has no instances. The\n",
    "`scitype` function is used to articulate MLJ's convention about how\n",
    "different machine types will be interpreted by MLJ models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "scitype(3.141)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = [2.3, 4.5, 4.2, 1.8, 7.1]\n",
    "scitype(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix data which MLJ is interpreting incorrectly, we use the\n",
    "`coerce` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = [185, 153, 163, 114, 180]\n",
    "scitype(height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = coerce(height, Continuous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of data we would want interpreted as\n",
    "`OrderedFactor` but isn't:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_mark = [\"rotten\", \"great\", \"bla\",  missing, \"great\"]\n",
    "scitype(exam_mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_mark = coerce(exam_mark, OrderedFactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels(exam_mark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `levels!` to put the classes in the right order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels!(exam_mark, [\"rotten\", \"bla\", \"great\"])\n",
    "exam_mark[1] < exam_mark[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When subsampling, no levels are not lost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels(exam_mark[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on binary data.** There is no separate scientific type for binary\n",
    "data. Binary data is `OrderedFactor{2}` if it has an intrinsic\n",
    "\"true\" class (eg, \"pass\"/\"fail\") and `Multiclass{2}` otherwise (eg,\n",
    "\"male\"/\"female\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever it makes sense, MLJ Models generally expect two-dimensional\n",
    "data to be *tabular*. All the tabular formats implementing the\n",
    "[Tables.jl API](https://juliadata.github.io/Tables.jl/stable/) (see\n",
    "this\n",
    "[list](https://github.com/JuliaData/Tables.jl/blob/master/INTEGRATIONS.md))\n",
    "have a scientific type of `Table` and can be used with such models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest example of a table is a the julia native *column\n",
    "table*, which is just a named tuple of equal-length vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_table = (h=height, e=exam_mark, t=time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scitype(column_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the `Table{K}` type parameter `K` encodes the scientific\n",
    "types of the columns. (This is useful when comparing table scitypes\n",
    "with `<:`). To inspect the individual column scitypes, we use the\n",
    "`schema` method instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema(column_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are four other examples of tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_table = [(a=1, b=3.4),\n",
    "             (a=2, b=4.5),\n",
    "             (a=3, b=5.6)]\n",
    "schema(row_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DataFrames\n",
    "df = DataFrames.DataFrame(column_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "file = CSV.File(joinpath(DIR, \"data\", \"horse.csv\"));\n",
    "schema(file) # (triggers a file read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most MLJ models do not accept matrix in lieu of a table, but you can\n",
    "wrap a matrix as a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_table = MLJ.table(rand(2,3))\n",
    "schema(matrix_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood many algorithms convert tabular data to matrices. If\n",
    "your table is a wrapped matrix like the above, then the compiler\n",
    "will generally collapse the conversions to a no-op."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manipulating tabular data.** In this workshop we assume\n",
    "familiarity with some kind of tabular data container (although it is\n",
    "possible, in principle, to carry out the exercises without this.)\n",
    "For a quick start introduction to `DataFrames`, see [this\n",
    "tutorial](https://alan-turing-institute.github.io/DataScienceTutorials.jl/data/dataframe/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing scientific types in tabular data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show how we can correct the scientific types of data in tables,\n",
    "we introduce a cleaned up version of the UCI Horse Colic Data Set\n",
    "(the cleaning workflow is described\n",
    "[here](https://alan-turing-institute.github.io/DataScienceTutorials.jl/end-to-end/horse/#dealing_with_missing_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "file = CSV.File(joinpath(DIR, \"data\", \"horse.csv\"));\n",
    "horse = DataFrames.DataFrame(file); # convert to data frame without copying columns\n",
    "first(horse, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [the UCI\n",
    "docs](http://archive.ics.uci.edu/ml/datasets/Horse+Colic) we can\n",
    "surmise how each variable ought to be interpreted (a step in our\n",
    "workflow that cannot reliably be left to the computer):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variable                    | scientific type (interpretation)\n",
    "----------------------------|-----------------------------------\n",
    "`:surgery`                  | Multiclass\n",
    "`:age`                      | Multiclass\n",
    "`:rectal_temperature`       | Continuous\n",
    "`:pulse`                    | Continuous\n",
    "`:respiratory_rate`         | Continuous\n",
    "`:temperature_extremities`  | OrderedFactor\n",
    "`:mucous_membranes`         | Multiclass\n",
    "`:capillary_refill_time`    | Multiclass\n",
    "`:pain`                     | OrderedFactor\n",
    "`:peristalsis`              | OrderedFactor\n",
    "`:abdominal_distension`     | OrderedFactor\n",
    "`:packed_cell_volume`       | Continuous\n",
    "`:total_protein`            | Continuous\n",
    "`:outcome`                  | Multiclass\n",
    "`:surgical_lesion`          | OrderedFactor\n",
    "`:cp_data`                  | Multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how MLJ will actually interpret the data, as it is\n",
    "currently encoded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema(horse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first correction step, we can get MLJ to \"guess\" the\n",
    "appropriate fix, using the `autotype` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autotype(horse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is not perfect, but a step in the right direction, which\n",
    "we implement like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce!(horse, autotype(horse));\n",
    "schema(horse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All remaining `Count` data should be `Continuous`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce!(horse, Count => Continuous);\n",
    "schema(horse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll correct the remaining truant entries manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce!(horse,\n",
    "        :surgery               => Multiclass,\n",
    "        :age                   => Multiclass,\n",
    "        :mucous_membranes      => Multiclass,\n",
    "        :capillary_refill_time => Multiclass,\n",
    "        :outcome               => Multiclass,\n",
    "        :cp_data               => Multiclass);\n",
    "schema(horse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources for Part 1\n",
    "\n",
    "- From the MLJ manual:\n",
    "   - [A preview of data type specification in\n",
    "  MLJ](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/#A-preview-of-data-type-specification-in-MLJ-1)\n",
    "   - [Data containers and scientific types](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/#Data-containers-and-scientific-types-1)\n",
    "   - [Working with Categorical Data](https://alan-turing-institute.github.io/MLJ.jl/dev/working_with_categorical_data/)\n",
    "- [Summary](https://alan-turing-institute.github.io/MLJScientificTypes.jl/dev/#Summary-of-the-MLJ-convention-1) of the MLJ convention for representing scientific types\n",
    "- [MLJScientificTypes.jl](https://alan-turing-institute.github.io/MLJScientificTypes.jl/dev/)\n",
    "- From Data Science Tutorials:\n",
    "    - [Data interpretation: Scientific Types](https://alan-turing-institute.github.io/DataScienceTutorials.jl/data/scitype/)\n",
    "    - [Horse colic data](https://alan-turing-institute.github.io/DataScienceTutorials.jl/end-to-end/horse/)\n",
    "- [UCI Horse Colic Data Set](http://archive.ics.uci.edu/ml/datasets/Horse+Colic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises for Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to guess how each code snippet below will evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scitype(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"who\", \"why\", \"what\", \"when\"]\n",
    "scitype(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elscitype(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = (3.141, 42, \"how\")\n",
    "scitype(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scitype(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elscitype(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SparseArrays\n",
    "Asparse = sparse(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scitype(Asparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CategoricalArrays\n",
    "C1 = categorical(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scitype(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elscitype(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C2 = categorical(A, ordered=true)\n",
    "scitype(C2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [1, 2, missing, 4]\n",
    "scitype(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elscitype(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scitype(v[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you guess at the general behaviour of\n",
    "`scitype` with respect to tuples, abstract arrays and missing\n",
    "values? The answers are\n",
    "[here](https://github.com/alan-turing-institute/ScientificTypes.jl#2-the-scitype-and-scitype-methods)\n",
    "(ignore \"Property 1\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coerce the following vector to make MLJ recognize it as a vector of\n",
    "ordered factors (with an appropriate ordering):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = [\"good\", \"poor\", \"poor\", \"excellent\", missing, \"good\", \"excellent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 (fixing scitypes in a table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the scitypes for the [House Prices in King\n",
    "County](https://mlr3gallery.mlr-org.com/posts/2020-01-30-house-prices-in-king-county/)\n",
    "dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = CSV.File(joinpath(DIR, \"data\", \"house.csv\"));\n",
    "house = DataFrames.DataFrame(file); # convert to data frame without copying columns\n",
    "first(house, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Two features in the original data set have been deemed uninformative\n",
    "and dropped, namely `:id` and `:date`. The original feature\n",
    "`:yr_renovated` has been replaced by the `Bool` feature `is_renovated`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Selecting, Training and Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Goals:**\n",
    "> 1. Search MLJ's database of model metadata to identify model candidates for a supervised learning task.\n",
    "> 2. Evaluate the performance of a model on a holdout set using basic `fit!`/`predict` workflow.\n",
    "> 3. Evaluate performance using other resampling strategies, such as cross-validation, in one line, using `evaluate!`\n",
    "> 4. Plot a \"learning curve\", to inspect performance as a function of some model hyper-parameter, such as an iteration parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Hello World!\" of machine learning is to classify Fisher's\n",
    "famous iris data set. This time, we'll grab the data from\n",
    "[OpenML](https://www.openml.org):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = OpenML.load(61); # a row table\n",
    "iris = DataFrames.DataFrame(iris);\n",
    "first(iris, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal.** To build and evaluate models for predicting the\n",
    "`:class` variable, given the four remaining measurement variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Inspect and fix scientific types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce!(iris, :class => Multiclass);\n",
    "schema(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Split data into input and target parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we split the data into target and input features, which\n",
    "is needed for MLJ supervised models. We randomize the data at the\n",
    "same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = unpack(iris, ==(:class), name->true; rng=123);\n",
    "scitype(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do `?unpack` to learn more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@doc unpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On searching for a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to see *all* models (not immediately useful):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitchen_sink = models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry contains metadata for a model whose defining code is not yet loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = kitchen_sink[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetscitype = meta.target_scitype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scitype(y) <: targetscitype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this model won't do. Let's  find all pure julia classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt(meta) = AbstractVector{Finite} <: meta.target_scitype &&\n",
    "        meta.is_pure_julia\n",
    "models(filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all models with \"Classifier\" in `name` (or `docstring`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models(\"Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all (supervised) models that match my data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models(matching(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Select and instantiate a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = @load NeuralNetworkClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLJ a *model* is just a struct containing hyper-parameters, and\n",
    "that's all. A model does not store *learned* parameters. Models are\n",
    "mutable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.epochs = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And all models have a key-word constructor that works once `@load`\n",
    "has been performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuralNetworkClassifier(epochs=12) == model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On fitting, predicting, and inspecting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLJ a model and training/validation data are typically bound\n",
    "together in a machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach = machine(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A machine stores *learned* parameters, among other things. We'll\n",
    "train this machine on 70% of the data and evaluate on a 30% holdout\n",
    "set. Let's start by dividing all row indices into `train` and `test`\n",
    "subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = partition(eachindex(y), 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(mach, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, one can inspect the learned parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_params(mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything else the user might be interested in is accessed from the\n",
    "training *report*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machines remember the last set of hyper-parameters used during fit,\n",
    "which, in the case of iterative models, allows for a warm restart of\n",
    "computations in the case that only the iteration parameter is\n",
    "increased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.epochs = model.epochs + 4\n",
    "fit!(mach, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default (for this particular model) we can also increase\n",
    "`:learning_rate` without triggering a cold restart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.epochs = model.epochs + 4\n",
    "model.optimiser.eta = 10*model.optimiser.eta\n",
    "fit!(mach, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, change any other parameter and training will restart from\n",
    "scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lambda = 0.001\n",
    "fit!(mach, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train silently for a total of 50 epochs, and look at a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.epochs = 50\n",
    "fit!(mach, rows=train)\n",
    "yhat = predict(mach, X[test,:]); # or predict(mach, rows=test)\n",
    "yhat[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info(model).prediction_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**:\n",
    "- In MLJ, a model that can predict probabilities (and not just point values) will do so by default. (These models have supertype `Proababilistic`, while point-estimate predictors have supertype `Deterministic`.)\n",
    "- For most probabilistic predictors, the predicted object is a `Distributions.Distribution` object, supporting the `Distributions.jl` [API](https://juliastats.org/Distributions.jl/latest/extends/#Create-a-Distribution-1) for such objects. In particular, the methods `rand`,  `pdf`, `mode`, `median` and `mean` will apply, where appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to obtain the probability of \"Iris-virginica\" in the first test\n",
    "prediction, we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf(yhat[1], \"Iris-virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the most likely observation, we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode(yhat[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be broadcast over multiple predictions in the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast(pdf, yhat[1:4], \"Iris-versicolor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode.(yhat[1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, alternatively, you can use the `predict_mode` operation instead\n",
    "of `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_mode(mach, X[test,:])[1:4] # or predict_mode(mach, rows=test)[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more conventional matrix of probabilities you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = levels(y)\n",
    "pdf(yhat, L)[1:4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in a typical MLJ workflow, this is not as useful as you\n",
    "might imagine. In particular, all probablistic performance measures\n",
    "in MLJ expect distribution objects in their first slot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy(yhat, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply a deterministic measure, we first need to obtain point-estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassification_rate(mode.(yhat), y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note in passing that there is also a search tool for measures\n",
    "analogous to `models`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures(matching(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Evaluate the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, MLJ provides boilerplate code for carrying out a model\n",
    "evaluation with a lot less fuss. Let's repeat the performance\n",
    "evaluation above and add an extra measure, `brier_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate!(mach, resampling=Holdout(fraction_train=0.7),\n",
    "          measures=[cross_entropy, brier_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or applying cross-validation instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, brier_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, Monte-Carlo cross-validation (cross-validation repeated\n",
    "randomizied folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = evaluate!(mach, resampling=CV(nfolds=6, rng=123),\n",
    "                repeats=3,\n",
    "              measures=[cross_entropy, brier_score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can access the following properties of the output `e` of an\n",
    "evaluation: `measure`, `measurement`, `per_fold` (measurement for\n",
    "each fold) and `per_observation` (measurement per observation, if\n",
    "reported)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally note that you can restrict the rows of observations from\n",
    "which train and test folds are drawn, by specifying `rows=...`. For\n",
    "example, imagining the last 30% of target observations are `missing`\n",
    "you might have a workflow like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = partition(eachindex(y), 0.7)\n",
    "mach = machine(model, X, y)\n",
    "evaluate!(mach, resampling=CV(nfolds=6),\n",
    "          measures=[cross_entropy, brier_score],\n",
    "          rows=train)     # cv estimate, resampling from `train`\n",
    "fit!(mach, rows=train)    # re-train using all of `train` observations\n",
    "predict(mach, rows=test); # and predict missing targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model is an iterative one, we might want to inspect the\n",
    "out-of-sample performance as a function of the iteration\n",
    "parameter. For this we can use the `learning_curve` function (which,\n",
    "incidentally can be applied to any model hyper-parameter). This\n",
    "starts by defining a one-dimensional range object for the parameter\n",
    "(more on this when we discuss tuning in Part 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = range(model, :epochs, lower=1, upper=60, scale=:log)\n",
    "curve = learning_curve(mach,\n",
    "                       range=r,\n",
    "                       resampling=Holdout(fraction_train=0.7), # (default)\n",
    "                       measure=cross_entropy)\n",
    "\n",
    "using Plots\n",
    "pyplot(size=(490,300))\n",
    "plt=plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(plt, \"epochs\")\n",
    "ylabel!(plt, \"cross entropy on holdout set\")\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will return to learning curves when we look at tuning in Part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources for Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the MLJ manual:\n",
    "    - [Getting Started](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/)\n",
    "    - [Model Search](https://alan-turing-institute.github.io/MLJ.jl/dev/model_search/)\n",
    "    - [Evaluating Performance](https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/) (using `evaluate!`)\n",
    "    - [Learning Curves](https://alan-turing-institute.github.io/MLJ.jl/dev/learning_curves/)\n",
    "    - [Performance Measures](https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/) (loss functions, scores, etc)\n",
    "- From Data Science Tutorials:\n",
    "    - [Choosing and evaluating a model](https://alan-turing-institute.github.io/DataScienceTutorials.jl/getting-started/choosing-a-model/)\n",
    "    - [Fit, predict, transform](https://alan-turing-institute.github.io/DataScienceTutorials.jl/getting-started/fit-and-predict/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises for Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Identify all supervised MLJ models that can be applied (without\n",
    "type coercion or one-hot encoding) to a supervised learning problem\n",
    "with input features `X4` and target `y4` defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Distributions\n",
    "poisson = Distributions.Poisson\n",
    "\n",
    "age = 18 .+ 60*rand(10);\n",
    "salary = coerce(rand([:small, :big, :huge], 10), OrderedFactor);\n",
    "levels!(salary, [:small, :big, :huge]);\n",
    "X4 = DataFrames.DataFrame(age=age, salary=salary)\n",
    "\n",
    "n_devices(salary) = salary > :small ? rand(poisson(1.3)) : rand(poisson(2.9))\n",
    "y4 = [n_devices(row.salary) for row in eachrow(X4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) What models can be applied if you coerce the salary to a\n",
    "`Continuous` scitype?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5 (unpack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluating the following ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (a = [1, 2, 3, 4],\n",
    "     b = rand(4),\n",
    "     c = rand(4),\n",
    "     d = coerce([\"male\", \"female\", \"female\", \"male\"], OrderedFactor));\n",
    "pretty(data)\n",
    "\n",
    "using Tables\n",
    "y, X, w = unpack(data, ==(:a),\n",
    "                 name -> elscitype(Tables.getcolumn(data, name)) == Continuous,\n",
    "                 name -> true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...attempt to guess the evaluations of the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6 (first steps in modelling Horse Colic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Suppose we want to use predict the `:outcome` variable in the\n",
    "Horse Colic study introduced in Part 1, based on the remaining\n",
    "variables that are `Continuous` (one-hot encoding categorical\n",
    "variables is discussed later in Part 3) *while ignoring the others*.\n",
    "Extract from the `horse` data set (defined in Part 1) appropriate\n",
    "input features `X` and target variable `y`. (Do not, however,\n",
    "randomize the observations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Create a 70:30 `train`/`test` split of the data and train a\n",
    "`LogisticClassifier` model, from the `MLJLinearModels` package, on\n",
    "the `train` rows. Use `lambda=100` and default values for the\n",
    "other hyper-parameters. (Although one would normally standardize\n",
    "(whiten) the continuous features for this model, do not do so here.)\n",
    "After training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (i) Recalling that a logistic classifier (aka logistic regressor) is\n",
    "  a linear-based model learning a *vector* of coefficients for each\n",
    "  feature (one coefficient for each target class), use the\n",
    "  `fitted_params` method to find this vector of coefficients in the\n",
    "  case of the `:pulse` feature. (To convert a vector of pairs `v =\n",
    "  [x1 => y1, x2 => y2, ...]` into a dictionary, do `Dict(v)`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (ii) Evaluate the `cross_entropy` performance on the `test`\n",
    "  observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- &star;(iii) In how many `test` observations does the predicted\n",
    "  probablility of the observed class exceed 50%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (iv) Find the `misclassification_rate` in the `test`\n",
    "  set. (*Hint.* As this measure is deterministic, you will either\n",
    "  need to broadcast `mode` or use `predict_mode` instead of\n",
    "  `predict`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Instead use a `RandomForestClassifier` model from the\n",
    "    `DecisionTree` package and:\n",
    "\n",
    "- (i) Generate an appropriate learning curve to convince yourself\n",
    "  that out-of-sample estimates of the `cross_entropy` loss do not\n",
    "  substatially improve for `n_trees > 50`. Use default values for\n",
    "  all other hyper-parameters, and feel free to use all available\n",
    "  data to generate the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (ii) Fix `n_trees=90` and use `evaluate!` to obtain a 9-fold\n",
    "  cross-validation estimate of the `cross_entropy`, restricting\n",
    "  sub-sampling to the `train` observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (iii) Now use *all* available data but set\n",
    "  `resampling=Holdout(fraction_train=0.7)` to obtain a score you can\n",
    "  compare with the `KNNClassifier` in part (b)(iii). Which model is\n",
    "  better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Transformers and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised models, which receive no target `y` during training,\n",
    "always have a `transform` operation. They sometimes also support an\n",
    "`inverse_transform` operation, with obvious meaning, and sometimes\n",
    "support a `predict` operation (see the clustering example discussed\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/#Transformers-that-also-predict-1)).\n",
    "Otherwise, they are handled much like supervised models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple standardization example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand(100);\n",
    "@show mean(x) std(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UnivariateStandardizer() # a built-in model\n",
    "mach = machine(model, x)\n",
    "fit!(mach)\n",
    "x̂ = transform(mach, x);\n",
    "@show mean(x̂) std(x̂);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular model has an `inverse_transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_transform(mach, x̂) ≈ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-encoding the King County House data as continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further illustrations of tranformers, let's re-encode *all* of the\n",
    "King County House input features (see [Ex\n",
    "3](#ex-3-fixing-scitypes-in-a-table)) into a set of `Continuous`\n",
    "features. We do this with the `ContinousEncoder` model, which, by\n",
    "default, will:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one-hot encode all `Multiclass` features\n",
    "- coerce all `OrderedFactor` features to `Continuous` ones\n",
    "- coerce all `Count` features to `Continuous` ones (there aren't any)\n",
    "- drop any remaining non-Continuous features (none of these either)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we reload the data and fix the scitypes (Exercise 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = CSV.File(joinpath(DIR, \"data\", \"house.csv\"));\n",
    "house = DataFrames.DataFrame(file)\n",
    "coerce!(house, autotype(file))\n",
    "coerce!(house, Count => Continuous, :zipcode => Multiclass);\n",
    "schema(house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = unpack(house, ==(:price), name -> true, rng=123);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the unsupervised model (transformer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ContinuousEncoder() # a built-in model; no need to @load it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bind the model to the data and fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach = machine(encoder, X) |> fit!;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform and inspect the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcont = transform(mach, X);\n",
    "schema(Xcont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to list all of MLJ's unsupervised models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models(m->!m.is_supervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some commonly used ones are built-in (do not require `@load`ing):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model type                  | does what?\n",
    "----------------------------|----------------------------------------------\n",
    "ContinuousEncoder | transform input to a table of `Continuous` features (see above)\n",
    "FeatureSelector | retain or dump selected features\n",
    "FillImputer | impute missing values\n",
    "OneHotEncoder | one-hot encoder `Multiclass` (and optionally `OrderedFactor`) features\n",
    "Standardizer | standardize (whiten) the `Continuous` features in a table\n",
    "UnivariateBoxCoxTransformer | apply a learned Box-Cox transformation to a vector\n",
    "UnivariateDiscretizer | discretize a `Continuous` vector, and hence render its elscityp `OrderedFactor`\n",
    "UnivariateStandardizer| standardize (whiten) a `Continuous` vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to \"dynamic\" transformers (ones that learn something\n",
    "from the data and must be `fit!`) users can wrap ordinary functions\n",
    "as transformers, and such *static* transformers can depend on\n",
    "parameters, like the dynamic ones. See\n",
    "[here](https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/#Static-transformers-1)\n",
    "for how to define your own static transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length(schema(Xcont).names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that additionally we'd like to reduce the dimension of\n",
    "our data.  A model that will do this is `PCA` from\n",
    "`MultivariateStats`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = @load PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, rather simply repeating the workflow above, applying the new\n",
    "transformation to `Xcont`, we can combine both the encoding and the\n",
    "dimension-reducing models into a single model, known as a\n",
    "*pipeline*. While MLJ offers a powerful interface for composing\n",
    "models in a variety of ways, we'll stick to these simplest class of\n",
    "composite models for now. The easiest way to construct them is using\n",
    "the `@pipeline` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = @pipeline encoder reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `pipe` is an *instance* of an automatically generated\n",
    "type (called `Pipeline<some digits>`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model behaves like any other transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach = machine(pipe, X) |> fit!;\n",
    "Xsmall = transform(mach, X)\n",
    "schema(Xsmall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to combine this pre-processing with ridge regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgs = @load RidgeRegressor pkg=MLJLinearModels\n",
    "pipe2 = @pipeline encoder reducer rgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our pipeline is a supervised model, instead of a transformer,\n",
    "whose performance we can evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach = machine(pipe2, X, y) |> fit!\n",
    "evaluate!(mach, measure=mae, resampling=Holdout()) # CV(nfolds=6) is default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of composite models is \"smart\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now notice what happens if we train on all the data, then change a\n",
    "regressor hyper-parameter and retrain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2.ridge_regressor.lambda = 0.1\n",
    "fit!(mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second time only the ridge regressor is retrained!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutate a hyper-parameter of the `PCA` model and every model except\n",
    "the `ContinuousEncoder` (which comes before it will be retrained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2.pca.pratio = 0.9999\n",
    "fit!(mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting composite models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot syntax used above to change the values of *nested*\n",
    "hyper-parameters is also useful when inspecting the learned\n",
    "parameters and report generated when training a composite model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_params(mach).ridge_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(mach).pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating target transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, suppose that instead of using the raw `:price` as the\n",
    "training target, we want to use the log-price (a common practice in\n",
    "dealing with house price data). However, suppose that we still want\n",
    "to report final *predictions* on the original linear scale (and use\n",
    "these for evaluation purposes). Then we supply appropriate functions\n",
    "to key-word arguments `target` and `inverse`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll overload `log` and `exp` for broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.log(v::AbstractArray) = log.(v)\n",
    "Base.exp(v::AbstractArray) = exp.(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the new pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe3 = @pipeline encoder reducer rgs target=log inverse=exp\n",
    "mach = machine(pipe3, X, y)\n",
    "evaluate!(mach, measure=mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLJ will also allow you to insert *learned* target\n",
    "transformations. For example, we might want to apply\n",
    "`UnivariateStandardizer()` to the target, to standarize it, or\n",
    "`UnivariateBoxCoxTransformer()` to make it look Gaussian. Then\n",
    "instead of specifying a *function* for `target`, we specify a\n",
    "unsupervised *model* (or model type). One does not specify `inverse`\n",
    "because only models implementing `inverse_transform` are\n",
    "allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which of these two options results in a better outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = UnivariateBoxCoxTransformer(n=20)\n",
    "stand = UnivariateStandardizer()\n",
    "\n",
    "pipe4 = @pipeline encoder reducer rgs target=box\n",
    "mach = machine(pipe4, X, y)\n",
    "evaluate!(mach, measure=mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe4.target = stand\n",
    "evaluate!(mach, measure=mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources for Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the MLJ manual:\n",
    "    - [Transformers and other unsupervised models](https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/)\n",
    "    - [Linear pipelines](https://alan-turing-institute.github.io/MLJ.jl/dev/composing_models/#Linear-pipelines-1)\n",
    "- From Data Science Tutorials:\n",
    "    - [Composing models](https://alan-turing-institute.github.io/DataScienceTutorials.jl/getting-started/composing-models/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises for Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the Horse Colic classification problem considered in\n",
    "Exercise 6, but with all features, `Finite` and `Infinite`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = unpack(horse, ==(:outcome), name -> true);\n",
    "schema(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Define a pipeline that:\n",
    "    - uses `Standardizer` to ensure that features that are already\n",
    "      continuous are centred at zero and have unit variance\n",
    "    - re-encodes the full set of features as `Continuous`, using\n",
    "      `ContinuousEncoder`\n",
    "    - uses the `KMeans` clustering model from `Clustering.jl`\n",
    "      to reduce the dimension of the feature space to `k=10`.\n",
    "    - trains a `EvoTreeClassifier` (a gradient tree boosting\n",
    "       algorithm in `EvoTrees.jl`) on the reduced data, using\n",
    "       `nrounds=50` and default values for the other\n",
    "       hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Evaluate the pipeline on all data, using 6-fold cross-validation\n",
    "and `cross_entropy` loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&star;(c) Plot a learning curve which examines the effect on this loss\n",
    "as the tree booster parameter `max_depth` varies from 2 to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Tuning hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = range(pipe3, :(ridge_regressor.lambda), lower = 1e-6, upper=10, scale=:log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're curious, you can see what `lambda` values this range will\n",
    "generate for a given resolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator(r, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions to exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality = coerce(quality, OrderedFactor);\n",
    "levels!(quality, [\"poor\", \"good\", \"excellent\"]);\n",
    "elscitype(quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce!(house, autotype(house));\n",
    "schema(house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the \"sqft\" fields refer to \"square feet\" so are\n",
    "really `Continuous`. We'll regard `:yr_built` (the other `Count`\n",
    "variable above) as `Continuous` as well. So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce!(house, Count => Continuous);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `:zipcode` should not be ordered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce!(house, :zipcode => Multiclass);\n",
    "schema(house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`:bathrooms` looks like it has a lot of levels, but on further\n",
    "inspection we see why, and `OrderedFactor` remains appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import StatsBase.countmap\n",
    "countmap(house.bathrooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4 solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are *no* models that apply immediately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models(matching(X4, y4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y4 = coerce(y4, Continuous);\n",
    "models(matching(X4, y4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6 solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = unpack(horse,\n",
    "              ==(:outcome),\n",
    "              name -> elscitype(Tables.getcolumn(horse, name)) == Continuous);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6(b)(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = @load LogisticClassifier pkg=MLJLinearModels;\n",
    "model.lambda = 100\n",
    "mach = machine(model, X, y)\n",
    "fit!(mach, rows=train)\n",
    "fitted_params(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_given_feature = Dict(fitted_params(mach).coefs)\n",
    "coefs_given_feature[:pulse]\n",
    "\n",
    "#6(b)(ii)\n",
    "\n",
    "yhat = predict(mach, rows=test); # or predict(mach, X[test,:])\n",
    "err = cross_entropy(yhat, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6(b)(iii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted probabilities of the actual observations in the test\n",
    "are given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = broadcast(pdf, yhat, y[test]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of times this probability exceeds 50% is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n50 = filter(x -> x > 0.5, p) |> length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, as a proportion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n50/length(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6(b)(iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassification_rate(mode.(yhat), y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6(c)(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = @load RandomForestClassifier pkg=DecisionTree\n",
    "mach = machine(model, X, y)\n",
    "evaluate!(mach, resampling=CV(nfolds=6), measure=cross_entropy)\n",
    "\n",
    "r = range(model, :n_trees, lower=10, upper=70, scale=:log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since random forests are inherently randomized, we generate multiple\n",
    "curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = plot()\n",
    "for i in 1:4\n",
    "    curve = learning_curve(mach,\n",
    "                           range=r,\n",
    "                           resampling=Holdout(),\n",
    "                           measure=cross_entropy)\n",
    "    plt=plot!(curve.parameter_values, curve.measurements)\n",
    "end\n",
    "xlabel!(plt, \"n_trees\")\n",
    "ylabel!(plt, \"cross entropy\")\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6(c)(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate!(mach, resampling=CV(nfolds=9),\n",
    "                measure=cross_entropy,\n",
    "                rows=train).measurement[1]\n",
    "\n",
    "model.n_trees = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6(c)(iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_forest = evaluate!(mach, resampling=Holdout(),\n",
    "                       measure=cross_entropy).measurement[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@load KMeans pkg=Clustering\n",
    "@load EvoTreeClassifier\n",
    "pipe = @pipeline(Standardizer,\n",
    "                 ContinuousEncoder,\n",
    "                 KMeans(k=10),\n",
    "                 EvoTreeClassifier(nrounds=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach = machine(pipe, X, y)\n",
    "evaluate!(mach, resampling=CV(nfolds=6), measure=cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = range(pipe, :(evo_tree_classifier.max_depth), lower=1, upper=10)\n",
    "\n",
    "curve = learning_curve(mach,\n",
    "                       range=r,\n",
    "                       resampling=CV(nfolds=6),\n",
    "                       measure=cross_entropy)\n",
    "\n",
    "plt = plot(curve.parameter_values, curve.measurements)\n",
    "xlabel!(plt, \"max_depth\")\n",
    "ylabel!(plt, \"CV estimate of cross entropy\")\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a second curve using a different random seed for the booster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.evo_tree_classifier.seed = 123\n",
    "curve = learning_curve(mach,\n",
    "                       range=r,\n",
    "                       resampling=CV(nfolds=6),\n",
    "                       measure=cross_entropy)\n",
    "plot!(curve.parameter_values, curve.measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
