{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-of-the-art model composition in MLJ (Machine Learning in Julia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script we use [model\n",
    "stacking](https://alan-turing-institute.github.io/DataScienceTutorials.jl/getting-started/stacking/)\n",
    "to demonstrate the ease with which machine learning models can be\n",
    "combined in sophisticated ways using MLJ. In the future MLJ will\n",
    "have a canned version of stacking. For now we show how to stack\n",
    "using MLJ's generic model composition syntax, which is an extension\n",
    "of the normal fit/predict syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = @__DIR__\n",
    "include(joinpath(DIR, \"setup.jl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking is hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Model\n",
    "stacking](https://alan-turing-institute.github.io/DataScienceTutorials.jl/getting-started/stacking/),\n",
    "popular in Kaggle data science competitions, is a sophisticated way\n",
    "to blend the predictions of multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the python toolbox\n",
    "[scikit-learn](https://scikit-learn.org/stable/) (or its [julia\n",
    "wrap](https://github.com/cstjean/ScikitLearn.jl)) you can use\n",
    "pipelines to combine composite models in simple ways but (automated)\n",
    "stacking is beyond its capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One python alternative is to use\n",
    "[vecstack](https://github.com/vecxoz/vecstack). The [core\n",
    "algorithm](https://github.com/vecxoz/vecstack/blob/master/vecstack/core.py)\n",
    "is about eight pages (without the scikit-learn interface):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](vecstack.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking is easy (in MLJ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MLJ's [generic model composition\n",
    "API](https://alan-turing-institute.github.io/MLJ.jl/dev/composing_models/)\n",
    "you can build a stack in about a page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the complete code needed to define a new model type that\n",
    "stacks two base regressors and one adjudicator in MLJ.  Here we use\n",
    "three folds to create the base-learner [out-of-sample\n",
    "predictions](https://alan-turing-institute.github.io/DataScienceTutorials.jl/getting-started/stacking/)\n",
    "to make it easier to read. You can make this generic with little fuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "\n",
    "folds(data, nfolds) =\n",
    "    partition(1:nrows(data), (1/nfolds for i in 1:(nfolds-1))...);\n",
    "\n",
    "model1 = @load LinearRegressor pkg=MLJLinearModels\n",
    "model2 = @load LinearRegressor pkg=MLJLinearModels\n",
    "judge = @load LinearRegressor pkg=MLJLinearModels\n",
    "\n",
    "X = source()\n",
    "y = source()\n",
    "\n",
    "folds(X::AbstractNode, nfolds) = node(XX->folds(XX, nfolds), X)\n",
    "MLJ.restrict(X::AbstractNode, f::AbstractNode, i) =\n",
    "    node((XX, ff) -> restrict(XX, ff, i), X, f);\n",
    "MLJ.corestrict(X::AbstractNode, f::AbstractNode, i) =\n",
    "    node((XX, ff) -> corestrict(XX, ff, i), X, f);\n",
    "\n",
    "f = folds(X, 3)\n",
    "\n",
    "m11 = machine(model1, corestrict(X, f, 1), corestrict(y, f, 1))\n",
    "m12 = machine(model1, corestrict(X, f, 2), corestrict(y, f, 2))\n",
    "m13 = machine(model1, corestrict(X, f, 3), corestrict(y, f, 3))\n",
    "\n",
    "y11 = predict(m11, restrict(X, f, 1));\n",
    "y12 = predict(m12, restrict(X, f, 2));\n",
    "y13 = predict(m13, restrict(X, f, 3));\n",
    "\n",
    "m21 = machine(model2, corestrict(X, f, 1), corestrict(y, f, 1))\n",
    "m22 = machine(model2, corestrict(X, f, 2), corestrict(y, f, 2))\n",
    "m23 = machine(model2, corestrict(X, f, 3), corestrict(y, f, 3))\n",
    "\n",
    "y21 = predict(m21, restrict(X, f, 1));\n",
    "y22 = predict(m22, restrict(X, f, 2));\n",
    "y23 = predict(m23, restrict(X, f, 3));\n",
    "\n",
    "y1_oos = vcat(y11, y12, y13);\n",
    "y2_oos = vcat(y21, y22, y23);\n",
    "\n",
    "X_oos = MLJ.table(hcat(y1_oos, y2_oos))\n",
    "\n",
    "m_judge = machine(judge, X_oos, y)\n",
    "\n",
    "m1 = machine(model1, X, y)\n",
    "m2 = machine(model2, X, y)\n",
    "\n",
    "y1 = predict(m1, X);\n",
    "y2 = predict(m2, X);\n",
    "\n",
    "X_judge = MLJ.table(hcat(y1, y2))\n",
    "yhat = predict(m_judge, X_judge)\n",
    "\n",
    "@from_network machine(Deterministic(), X, y; predict=yhat) begin\n",
    "    mutable struct MyStack\n",
    "        regressor1=model1\n",
    "        regressor2=model2\n",
    "        judge=judge\n",
    "    end\n",
    "end\n",
    "\n",
    "my_stack = MyStack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the curious: Only the last block defines the new model type. The\n",
    "rest defines a *[learning network]()* - a kind of working prototype\n",
    "or blueprint for the type. If the source nodes `X` and `y` wrap some\n",
    "data (instead of nothing) then the network can be trained and tested\n",
    "as you build it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composition plays well with other work-flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not include standardization of inputs and target (with\n",
    "post-prediction inversion) in our stack. However, we can add these\n",
    "now, using MLJ's canned pipeline composition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = @pipeline Standardizer my_stack target=Standardizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to change a base learner and adjudicator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.my_stack.regressor2 = @load DecisionTreeRegressor pkg=DecisionTree\n",
    "pipe.my_stack.judge = @load KNNRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want a CV estimate of performance of the complete model on some data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = @load_boston;\n",
    "mach = machine(pipe, X, y)\n",
    "evaluate!(mach, resampling=CV(), measure=[mae, rms])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to inspect the learned parameters of the adjudicator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp =  fitted_params(mach);\n",
    "fp.my_stack.judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the first base-learner of the stack? There are four sets\n",
    "of learned parameters!  One for each fold to make an out-of-sample\n",
    "prediction, and one trained on all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.my_stack.regressor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp.my_stack.regressor1[1].coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to tune multiple (nested) hyperparameters in the stack? Tuning is a\n",
    "model wrapper (for better composition!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = range(pipe, :(my_stack.regressor2.max_depth), lower = 1, upper = 25)\n",
    "r2 = range(pipe, :(my_stack.judge.K), lower=1, origin=10, unit=10)\n",
    "\n",
    "import Distributions.Poisson\n",
    "\n",
    "tuned_pipe = TunedModel(model=pipe,\n",
    "                         ranges=[r1, (r2, Poisson)],\n",
    "                         tuning=RandomSearch(),\n",
    "                         resampling=CV(),\n",
    "                         measure=rms,\n",
    "                         n=100)\n",
    "mach = machine(tuned_pipe, X, y) |> fit!\n",
    "best_model = fitted_params(mach).best_model\n",
    "K = fitted_params(mach).best_model.my_stack.judge.K;\n",
    "max_depth = fitted_params(mach).best_model.my_stack.regressor2.max_depth\n",
    "@show K max_depth;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize tuning results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "pyplot()\n",
    "plot(mach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
